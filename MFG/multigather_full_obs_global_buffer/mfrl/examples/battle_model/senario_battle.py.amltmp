"""
# Original Work Copyright © 2018-present, Mean Field Reinforcement Learning (https://github.com/mlii/mfrl). 
# All rights reserved.
# Modifications (c) 2020-present, Royal Bank of Canada.
# All rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
"""


import random
import math
import numpy as np


def generate_map(env, map_size, food_handle, handles):
    width = map_size
    height = map_size

    init_num = map_size * map_size * 0.04

    gap = 3
    # left
    n = init_num
    side = int(math.sqrt(n)) * 2
    pos = [[], []]
    ct = 0
    for x in range(width//2 - gap - side, width//2 - gap - side + side, 2):
        for y in range((height - side)//2, (height - side)//2 + side, 2):
            pos[ct % 2].append([x, y])
        ct += 1
    env.add_agents(handles[0], method="custom", pos=pos[0])
    env.add_agents(handles[1], method="custom", pos=pos[1])


    center_x, center_y = map_size // 2, map_size // 2

    def add_square(pos, side, gap):
        side = int(side)
        for x in range(center_x - side//2, center_x + side//2 + 1, gap):
            pos.append([x, center_y - side//2])
            pos.append([x, center_y + side//2])
        for y in range(center_y - side//2, center_y + side//2 + 1, gap):
            pos.append([center_x - side//2, y])
            pos.append([center_x + side//2, y])


       # food
    pos = []
    add_square(pos, map_size * 0.65, 10)
    add_square(pos, map_size * 0.6,  10)
    add_square(pos, map_size * 0.55, 10)
    add_square(pos, map_size * 0.5,  4)
    add_square(pos, map_size * 0.45, 3)
    add_square(pos, map_size * 0.4, 1)
    add_square(pos, map_size * 0.3, 1)
    add_square(pos, map_size * 0.3 - 2, 1)
    add_square(pos, map_size * 0.3 - 4, 1)
    add_square(pos, map_size * 0.3 - 6, 1)
    env.add_agents(food_handle, method="custom", pos=pos)

        # legend
    legend = [
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],
        [1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,],
        [1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,],
        [1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,],
        [1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],
    ]

    org = [
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],
    ]

    def draw(base_x, base_y, scale, data):
        w, h = len(data), len(data[0])
        pos = []
        for i in range(w):
            for j in range(h):
                if data[i][j] == 1:
                    start_x = i * scale + base_x
                    start_y = j * scale + base_y
                    for x in range(start_x, start_x + scale):
                        for y in range(start_y, start_y + scale):
                            pos.append([y, x])

        env.add_agents(food_handle, method="custom", pos=pos)

    scale = 1
    w, h = len(legend), len(legend[0])
    offset = -3
    draw(offset + map_size // 2 - w // 2 * scale, map_size // 2 - h // 2 * scale, scale, legend)
    draw(offset + map_size // 2 - w // 2 * scale + len(legend), map_size // 2 - h // 2 * scale, scale, org)



def build_mean_field_action_space(env, handles, number_of_agent_types):
    mean_field_action = {}    
    for agent_num in range(number_of_agent_types):
        mean_field_action['agent_' + str(agent_num)] = [ np.zeros((1, env.get_action_space(handles[i])[0])) for i in range(number_of_agent_types)]
    
    mean_field_action['former_prob'] =  [ np.zeros((1, env.get_action_space(handles[i])[0])) for i in range(number_of_agent_types)]
    return mean_field_action


def build_mean_field_obs_space(env, handles, number_of_agent_types):
    mean_field_obs = {}
    mean_field_obs['former_obs'] = [ np.zeros((1, ) + env.get_view_space(handles[i])) for i in range(number_of_agent_types)]
    
    for agent_num in range(number_of_agent_types):
        mean_field_obs['agent_' + str(agent_num)] = [ np.zeros((1, ) + env.get_view_space(handles[i])) for i in range(number_of_agent_types)]


    return mean_field_obs

def build_mean_field_feature_space(env, handles, number_of_agent_types):
    mean_field_feature = {}
    mean_field_feature['former_feature'] = [ np.zeros((1, ) + env.get_feature_space(handles[i])) for i in range(number_of_agent_types)]
    
    for agent_num in range(number_of_agent_types):
        mean_field_feature['agent_' + str(agent_num)] = [ np.zeros((1, ) + env.get_feature_space(handles[i])) for i in range(number_of_agent_types)]


    return mean_field_feature


def get_mean_field_actions(mean_field_action, state, number_of_agent_types):
    

    for agent_num in range(number_of_agent_types):
        mean_field_action['agent_' + str(agent_num)]= [np.tile(mean_field_action['former_prob'][i], (len(state[agent_num][0]), 1)) for i in range(number_of_agent_types)]
    
    return mean_field_action


def get_mean_field_obs(mean_field_obs, state, number_of_agent_types):
    
    for agent_num in range(number_of_agent_types):
        # np.repeat(former_obs_prob[0], len(state[i][0]), axis=0)
        mean_field_obs['agent_' + str(agent_num)] = [np.repeat(mean_field_obs['former_obs'][i], len(state[agent_num][0]), axis=0) for i in range(number_of_agent_types)]
    
    return mean_field_obs


def get_mean_field_feature(mean_field_feature, state, number_of_agent_types):
    
    for agent_num in range(number_of_agent_types):
        mean_field_feature['agent_' + str(agent_num)] = [np.repeat(mean_field_feature['former_feature'][i], len(state[agent_num][0]), axis=0) for i in range(number_of_agent_types)]
    
    return mean_field_feature


def build_agent_list(model_list, adv_type_list, self_adversary_prob=0.1):
    
    # Choose random adversary
    adv_index = np.random.choice([x for x in range(1, len(model_list))])

    # In 20% of the cases
    if np.random.uniform(0, 1) <= self_adversary_prob:
        adv_index = 0

    agents = [
        {
            'model': model_list[0],
            'type': adv_type_list[0]
        },
        {
            'model': model_list[adv_index],
            'type': adv_type_list[adv_index]
        }
    ]
    return agents


def play_multi(env, n_round, map_size, max_steps, handles, model_list, adv_type_list, print_every, eps=1.0, render=False, train=False, tools=None):
    """play a round and train"""


    env.reset()
    generate_map(env, map_size, handles)
    
    step_ct = 0
    done = False

    n_group = len(handles)
    state = [None for _ in range(n_group)]


    acts = [0 for _ in range(n_group)]
    
    mean_obs = [0 for _ in range(n_group)]
    mean_feat = [0 for _ in range(n_group)]


    ids = [None for _ in range(n_group)]

    alives = [None for _ in range(n_group)]
    rewards = [None for _ in range(n_group)]
    nums = [env.get_num(handle) for handle in handles]
    max_nums = nums.copy()
    loss = [None for _ in range(n_group)]
    eval_q = [None for _ in range(n_group)]
    n_action = [env.get_action_space(handles[0])[0], env.get_action_space(handles[1])[0]]

    print("\n\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2}".format(n_round, eps, nums))
    mean_rewards = [[] for _ in range(n_group)]
    total_rewards = [[] for _ in range(n_group)]


    mean_field_action_space = build_mean_field_action_space(env=env, handles=handles, number_of_agent_types=2)
    mean_field_obs_space = build_mean_field_obs_space(env=env, handles=handles, number_of_agent_types=2)
    mean_field_feature_space = build_mean_field_feature_space(env=env, handles=handles, number_of_agent_types=2)
    agents = build_agent_list(model_list, adv_type_list)

    
    while not done and step_ct < max_steps:
        # take actions for every model
        for i in range(n_group):
            state[i] = list(env.get_observation(handles[i]))
            ids[i] = env.get_agent_id(handles[i])

        for i in range(n_group):
            mean_field_actions = get_mean_field_actions(mean_field_action_space, state, n_group)
            mean_field_obs = get_mean_field_obs(mean_field_obs_space, state, n_group)
            mean_field_feature = get_mean_field_feature(mean_field_feature_space, state, n_group)

            if agents[i]['type'] == 'GenQ_MFG':
                acts[i], mean_obs[i], mean_feat[i] = agents[i]['model'].act(
                    state=state[i],

                    prob0=mean_field_actions['agent_' + str(i)][0],
                    prob1=mean_field_actions['agent_' + str(i)][1],

                    type0_state0=mean_field_obs['agent_' + str(i)][0],
                    type0_state1=mean_field_feature['agent_'+ str(i)][0],
                    
                    type1_state0=mean_field_obs['agent_'+ str(i)][1],
                    type1_state1=mean_field_feature['agent_' + str(i)][1],
                    eps=eps
                )
            elif agents[i]['type'] == 'mtmfq':
                acts[i], mean_obs[i], mean_feat[i] = agents[i]['model'].act(state=state[i], prob0=mean_field_actions['agent_'+str(i)][0], prob1=mean_field_actions['agent_'+str(i)][1], prob2=mean_field_actions['agent_'+str(i)][0], prob3=mean_field_actions['agent_'+str(i)][1], eps=eps) 
            
            else:
                acts[i], mean_obs[i], mean_feat[i] = agents[i]['model'].act(state=state[i], prob=mean_field_actions['agent_'+str(i)][i], eps=eps) 


        for i in range(n_group):
            env.set_action(handles[i], acts[i])

        # simulate one step
        done = env.step()

        for i in range(n_group):
            rewards[i] = env.get_reward(handles[i])
            alives[i] = env.get_alive(handles[i])

        buffer = {
            'state': state[0], 'acts': acts[0], 'rewards': rewards[0],
            'alives': alives[0], 'ids': ids[0]
        }
        buffer['prob'] = np.tile(mean_field_actions['former_prob'][0], (len(state[0][0]), 1))
        buffer['prob0'] = mean_field_actions['agent_0'][0]
        buffer['prob1'] = mean_field_actions['agent_0'][1]
        
        # Extra
        buffer['prob2'] = mean_field_actions['agent_0'][0]
        buffer['prob3'] = mean_field_actions['agent_0'][1]

        buffer['type0_state0'] = mean_field_obs['agent_0'][0]
        buffer['type0_state1'] = mean_field_feature['agent_0'][0]

        buffer['type1_state0'] = mean_field_obs['agent_0'][1]
        buffer['type1_state1'] = mean_field_feature['agent_0'][1]

        
        if train:
            model_list[0].flush_buffer(**buffer)
            # model_list[1].flush_buffer(**buffer)
            # model_list[2].flush_buffer(**buffer)
            # model_list[3].flush_buffer(**buffer)
            # model_list[4].flush_buffer(**buffer)

        
        buffer = {
            'state': state[1], 'acts': acts[1], 'rewards': rewards[1],
            'alives': alives[1], 'ids': ids[1]
        }

        buffer['prob'] = np.tile(mean_field_actions['former_prob'][1], (len(state[1][0]), 1))
        buffer['prob0'] = mean_field_actions['agent_1'][0]
        buffer['prob1'] = mean_field_actions['agent_1'][1]
        
        # Extra
        buffer['prob2'] = mean_field_actions['agent_1'][0]
        buffer['prob3'] = mean_field_actions['agent_1'][1]

        buffer['type0_state0'] = mean_field_obs['agent_1'][0]
        buffer['type0_state1'] = mean_field_feature['agent_1'][0]

        buffer['type1_state0'] = mean_field_obs['agent_1'][1]
        buffer['type1_state1'] = mean_field_feature['agent_1'][1]


        if train:
            # models[1].flush_buffer(**buffer)
            # model_list[0].flush_buffer(**buffer)
            model_list[1].flush_buffer(**buffer)
            model_list[2].flush_buffer(**buffer)
            model_list[3].flush_buffer(**buffer)
            model_list[4].flush_buffer(**buffer)

        
        for i in range(n_group):
            mean_field_action_space['former_prob'][i] = np.mean(list(map(lambda x: np.eye(n_action[i])[x], acts[i])), axis=0, keepdims=True)
            mean_field_obs_space['former_obs'][i] = np.mean(state[i][0], axis=0, keepdims=True)
            mean_field_feature_space['former_feature'][i] = np.mean(state[i][1], axis=0, keepdims=True)
            

        # stat info
        nums = [env.get_num(handle) for handle in handles]

        for i in range(n_group):
            sum_reward = sum(rewards[i])
            rewards[i] = sum_reward / nums[i]
            mean_rewards[i].append(rewards[i])
            total_rewards[i].append(sum_reward)

        if render:
            env.render()

        # clear dead agents
        env.clear_dead()
        
        info = {"Ave-Reward": np.round(rewards, decimals=6), "NUM": nums}

        step_ct += 1

        if (step_ct % print_every == 0) or done == True:
            print("> step #{}, info: {}".format(step_ct, info))

    if train:
        loss[0] = model_list[0].train()
        loss[1] = model_list[1].train()
        loss[1] += model_list[2].train()
        loss[1] += model_list[3].train()
        loss[1] += model_list[4].train()
        loss[1] /= 3
        # loss[1] = models[1].train()
    

    for i in range(n_group):
        mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])
        total_rewards[i] = sum(total_rewards[i])

    return max_nums, nums, mean_rewards, total_rewards, loss



def battle(env, n_round, map_size, max_steps, handles, model_list, print_every, adv_type_list=None, eps=1.0, render=False, train=False):
    """play a round and train"""
    env.reset()
    generate_map(env, map_size, handles)

    step_ct = 0
    done = False

    n_group = len(handles)
    state = [None for _ in range(n_group)]
    acts = [None for _ in range(n_group)]
    ids = [None for _ in range(n_group)]

    loss = [0 for _ in range(n_group)]
    alives = [None for _ in range(n_group)]
    rewards = [None for _ in range(n_group)]
    nums = [env.get_num(handle) for handle in handles]
    max_nums = nums.copy()

    n_action = [env.get_action_space(handles[0])[0], env.get_action_space(handles[1])[0]]

    print("\n\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2}".format(n_round, eps, nums))
    mean_rewards = [[] for _ in range(n_group)]
    total_rewards = [[] for _ in range(n_group)]
    
    mean_obs = [None for _ in range(n_group)]
    mean_feat = [None for _ in range(n_group)]

    mean_field_action_space = build_mean_field_action_space(env=env, handles=handles, number_of_agent_types=2)
    mean_field_obs_space = build_mean_field_obs_space(env=env, handles=handles, number_of_agent_types=2)
    mean_field_feature_space = build_mean_field_feature_space(env=env, handles=handles, number_of_agent_types=2)
    agents = build_agent_list(model_list, adv_type_list)

    while not done and step_ct < max_steps:
        # take actions for every model
        for i in range(n_group):
            state[i] = list(env.get_observation(handles[i]))
            ids[i] = env.get_agent_id(handles[i])

        for i in range(n_group):
            mean_field_actions = get_mean_field_actions(mean_field_action_space, state, n_group)
            mean_field_obs = get_mean_field_obs(mean_field_obs_space, state, n_group)
            mean_field_feature = get_mean_field_feature(mean_field_feature_space, state, n_group)

            if agents[i]['type'] == 'GenQ_MFG':
                acts[i], mean_obs[i], mean_feat[i] = agents[i]['model'].act(
                    state=state[i],

                    prob0=mean_field_actions['agent_' + str(i)][0],
                    prob1=mean_field_actions['agent_' + str(i)][1],

                    type0_state0=mean_field_obs['agent_' + str(i)][0],
                    type0_state1=mean_field_feature['agent_'+ str(i)][0],
                    
                    type1_state0=mean_field_obs['agent_'+ str(i)][1],
                    type1_state1=mean_field_feature['agent_' + str(i)][1],
                    eps=eps,
                    greedy=True
                )
            elif agents[i]['type'] == 'mtmfq':
                acts[i], mean_obs[i], mean_feat[i] = agents[i]['model'].act(state=state[i], prob0=mean_field_actions['agent_'+str(i)][0], prob1=mean_field_actions['agent_'+str(i)][1], prob2=mean_field_actions['agent_'+str(i)][0], prob3=mean_field_actions['agent_'+str(i)][1], eps=eps) 
            
            else:
                acts[i], mean_obs[i], mean_feat[i] = agents[i]['model'].act(state=state[i], prob=mean_field_actions['agent_'+str(i)][i], eps=eps) 


        for i in range(n_group):
            env.set_action(handles[i], acts[i])

        # simulate one step
        done = env.step()

        for i in range(n_group):
            rewards[i] = env.get_reward(handles[i])
            alives[i] = env.get_alive(handles[i])

        
        for i in range(n_group):
            mean_field_action_space['former_prob'][i] = np.mean(list(map(lambda x: np.eye(n_action[i])[x], acts[i])), axis=0, keepdims=True)
            mean_field_obs_space['former_obs'][i] = np.mean(state[i][0], axis=0, keepdims=True)
            mean_field_feature_space['former_feature'][i] = np.mean(state[i][1], axis=0, keepdims=True)
        
        # stat info
        
        nums = [env.get_num(handle) for handle in handles]

        for i in range(n_group):
            sum_reward = sum(rewards[i])
            rewards[i] = sum_reward / nums[i]
            mean_rewards[i].append(rewards[i])
            total_rewards[i].append(sum_reward)

        if render:
            env.render()

        # clear dead agents
        env.clear_dead()
        nums = [env.get_num(handle) for handle in handles]

        info = {"Ave-Reward": np.round(rewards, decimals=6), "NUM": nums}

        step_ct += 1

        if step_ct % print_every == 0 or done:
            print("> step #{}, info: {}".format(step_ct, info))

    for i in range(n_group):
        mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])
        total_rewards[i] = sum(total_rewards[i])

    return max_nums, nums, mean_rewards, total_rewards, loss

